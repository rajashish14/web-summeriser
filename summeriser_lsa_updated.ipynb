{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c41fcd4e-0d6c-4857-b903-b90de9e4f117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31f66476-91c3-43c1-9ad2-6e0ac39a5581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\ASHISHI\n",
      "[nltk_data]     RANJAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\ASHISHI\n",
      "[nltk_data]     RANJAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\ASHISHI\n",
      "[nltk_data]     RANJAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure nltk data is downloaded\n",
    "nltk.data.path.append(r\"C:\\Users\\ASHISHI RANJAN\\AppData\\Roaming\\nltk_data\")\n",
    "nltk.download('punkt', force=True)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]  # Remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ba4f806-bf28-430f-a35e-f81f3359de48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASHISHI RANJAN\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.find('tokenizers/punkt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c6c2816-7ef2-44b2-b208-631083f215d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_text(text):\n",
    "#     # Tokenize text\n",
    "#     tokens = word_tokenize(text)\n",
    "\n",
    "#     # Remove stop words\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "#     # Lemmatize tokens\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "#     return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a7cc4-1c30-4004-81ba-9186d5c0cfec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb726fdf-400b-437e-8666-7c4cd3718317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def text_representation(text):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit([preprocess_text(text)])  # Fit TF-IDF\n",
    "    vectors = vectorizer.transform([preprocess_text(text)])  # Transform text to vector\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b19cfab2-8709-4ec4-9510-5567e08f0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sumy.parsers.plaintext import PlaintextParser\n",
    "# from sumy.nlp.tokenizers import Tokenizer\n",
    "# from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "# def summarize_text(text):\n",
    "#     parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "#     summarizer = LsaSummarizer()\n",
    "#     summary = summarizer(parser.document, 3)  # Number of sentences\n",
    "#     return \" \".join(str(sentence) for sentence in summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b108f05c-1b8f-406f-9b36-9bf949efe618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def summarize_text(text, min_words=50, max_words=200):\n",
    "    # Generate summary with BART while ensuring word constraints\n",
    "    raw_summary = summarizer(text, max_length=max_words, min_length=min_words, do_sample=False)[0]['summary_text']\n",
    "\n",
    "    # Trim summary to exact word range\n",
    "    words = raw_summary.split()\n",
    "    num_words = min(len(words), max(1, (min_words + max_words) // 2))  # Balanced output\n",
    "    final_summary = \" \".join(words[:num_words])\n",
    "\n",
    "    return final_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4467106-9a66-4770-9ddc-dd6b0c74b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_summary(summary):\n",
    "    # Remove extra spaces\n",
    "    summary = ' '.join(summary.split())\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c0171d2-9eae-45f0-8b2c-6f029bc0e1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Summary: NLTK (Natural Language Toolkit) is a popular Python library for natural language processing (NLP) It provides us various text processing libraries with a lot of test datasets. A variety of tasks can be performed using NLTK such as tokenizing, parse tree visualization, etc…\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"NLTK (Natural Language Toolkit) is a popular Python library for natural language processing (NLP). It provides us various text processing libraries with a lot of test datasets. A variety of tasks can be performed using NLTK such as tokenizing, parse tree visualization, etc… In this article, we will go through how we can set up NLTK in our system and use them for performing various NLP tasks during the text processing step. Tokenization:\n",
    "The breaking down of text into smaller units is called tokens. tokens are a small part of that text. If we have a sentence, the idea is to separate each word and build a vocabulary such that we can represent all words uniquely in a list. Numbers, words, etc.. all fall under tokens.\n",
    "\n",
    "Lower case conversion:\n",
    "We want our model to not get confused by seeing the same word with different cases like one starting with capital and one without and interpret both differently. So we convert all words into the lower case to avoid redundancy in the token list. Stop Words removal:\n",
    "When we use the features from a text to model, we will encounter a lot of noise. These are the stop words like the, he, her, etc… which don’t help us and, just be removed before processing for cleaner processing inside the model. With NLTK we can see all the stop words available in the English language.\n",
    "\n",
    "from nltk.corpus import stopwords\"\"\"\n",
    "summary = summarize_text(text)\n",
    "print(\"Final Summary:\", post_process_summary(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf13e69d-dbb8-475c-990e-f8dfd73a6d8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5Tokenizer\n\u001b[0;32m      5\u001b[0m t5_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m t5_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(t5_model_name)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5Tokenizer loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ASHISHI RANJAN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASHISHI RANJAN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import sentencepiece\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "t5_model_name = \"t5-large\"\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "print(\"T5Tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd220988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc6d767",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load T5 for long summary generation\u001b[39;00m\n\u001b[0;32m      7\u001b[0m t5_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m t5_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(t5_model_name)\n\u001b[0;32m      9\u001b[0m t5_model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(t5_model_name)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load BART for standard summarization\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASHISHI RANJAN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1666\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1666\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASHISHI RANJAN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1654\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1652\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, pipeline\n",
    "import openai\n",
    "import sentencepiece\n",
    "import torch\n",
    "\n",
    "# Load T5 for long summary generation\n",
    "t5_model_name = \"t5-large\"\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n",
    "\n",
    "# Load BART for standard summarization\n",
    "bart_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# OpenAI GPT-4 API key (Make sure to set up your key in an environment variable)\n",
    "OPENAI_API_KEY = \"your_openai_api_key_here\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "### 1️⃣ Function: Summarize a Long Text (Re-Summarization) ###\n",
    "def summarize_text_t5(text, min_words=1000, max_words=2000):\n",
    "    \"\"\"Generate a long summary directly from the original text using T5.\"\"\"\n",
    "    prompt = f\"summarize: {text} </s>\"\n",
    "    inputs = t5_tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    summary_ids = t5_model.generate(\n",
    "        **inputs, \n",
    "        max_length=max_words, \n",
    "        min_length=min_words, \n",
    "        length_penalty=2.0, \n",
    "        num_beams=4\n",
    "    )\n",
    "\n",
    "    summary = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "\n",
    "### 2️⃣ Function: Expand a Short Summary to Longer One ###\n",
    "def expand_summary_gpt4(summary, target_word_count=2000):\n",
    "    \"\"\"Expand an existing summary to a longer one using GPT-4.\"\"\"\n",
    "    prompt = f\"Expand the following summary to {target_word_count} words by adding details, examples, and explanations:\\n\\n{summary}\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are an expert at expanding summaries.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "### Gradio UI ###\n",
    "def summarizer_interface(text, min_words, max_words, expand_summary=False):\n",
    "    \"\"\"Handles both summarization and expansion based on user selection.\"\"\"\n",
    "    \n",
    "    # First, generate an initial summary\n",
    "    initial_summary = summarize_text_t5(text, min_words, max_words)\n",
    "    \n",
    "    # If the user wants to expand it further, pass it to GPT-4\n",
    "    if expand_summary:\n",
    "        final_summary = expand_summary_gpt4(initial_summary, max_words)\n",
    "        return final_summary\n",
    "    else:\n",
    "        return initial_summary\n",
    "\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=summarizer_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter Text to Summarize\"),\n",
    "        gr.Slider(500, 5000, step=100, label=\"Min Summary Length (words)\"),\n",
    "        gr.Slider(500, 5000, step=100, label=\"Max Summary Length (words)\"),\n",
    "        gr.Checkbox(label=\"Expand Summary with GPT-4\")\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Advanced Summarizer (Re-Summarization & Expansion)\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
