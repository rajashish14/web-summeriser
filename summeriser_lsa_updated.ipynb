{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c41fcd4e-0d6c-4857-b903-b90de9e4f117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31f66476-91c3-43c1-9ad2-6e0ac39a5581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\ASHISHI\n",
      "[nltk_data]     RANJAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\ASHISHI\n",
      "[nltk_data]     RANJAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\ASHISHI\n",
      "[nltk_data]     RANJAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure nltk data is downloaded\n",
    "nltk.data.path.append(r\"C:\\Users\\ASHISHI RANJAN\\AppData\\Roaming\\nltk_data\")\n",
    "nltk.download('punkt', force=True)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]  # Remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ba4f806-bf28-430f-a35e-f81f3359de48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASHISHI RANJAN\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.find('tokenizers/punkt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c6c2816-7ef2-44b2-b208-631083f215d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_text(text):\n",
    "#     # Tokenize text\n",
    "#     tokens = word_tokenize(text)\n",
    "\n",
    "#     # Remove stop words\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "#     # Lemmatize tokens\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "#     return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a7cc4-1c30-4004-81ba-9186d5c0cfec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb726fdf-400b-437e-8666-7c4cd3718317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def text_representation(text):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit([preprocess_text(text)])  # Fit TF-IDF\n",
    "    vectors = vectorizer.transform([preprocess_text(text)])  # Transform text to vector\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b19cfab2-8709-4ec4-9510-5567e08f0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sumy.parsers.plaintext import PlaintextParser\n",
    "# from sumy.nlp.tokenizers import Tokenizer\n",
    "# from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "# def summarize_text(text):\n",
    "#     parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "#     summarizer = LsaSummarizer()\n",
    "#     summary = summarizer(parser.document, 3)  # Number of sentences\n",
    "#     return \" \".join(str(sentence) for sentence in summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b108f05c-1b8f-406f-9b36-9bf949efe618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def summarize_text(text, min_words=50, max_words=200):\n",
    "    # Generate summary with BART while ensuring word constraints\n",
    "    raw_summary = summarizer(text, max_length=max_words, min_length=min_words, do_sample=False)[0]['summary_text']\n",
    "\n",
    "    # Trim summary to exact word range\n",
    "    words = raw_summary.split()\n",
    "    num_words = min(len(words), max(1, (min_words + max_words) // 2))  # Balanced output\n",
    "    final_summary = \" \".join(words[:num_words])\n",
    "\n",
    "    return final_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4467106-9a66-4770-9ddc-dd6b0c74b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_summary(summary):\n",
    "    # Remove extra spaces\n",
    "    summary = ' '.join(summary.split())\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c0171d2-9eae-45f0-8b2c-6f029bc0e1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Summary: NLTK (Natural Language Toolkit) is a popular Python library for natural language processing (NLP) It provides us various text processing libraries with a lot of test datasets. A variety of tasks can be performed using NLTK such as tokenizing, parse tree visualization, etc…\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"NLTK (Natural Language Toolkit) is a popular Python library for natural language processing (NLP). It provides us various text processing libraries with a lot of test datasets. A variety of tasks can be performed using NLTK such as tokenizing, parse tree visualization, etc… In this article, we will go through how we can set up NLTK in our system and use them for performing various NLP tasks during the text processing step. Tokenization:\n",
    "The breaking down of text into smaller units is called tokens. tokens are a small part of that text. If we have a sentence, the idea is to separate each word and build a vocabulary such that we can represent all words uniquely in a list. Numbers, words, etc.. all fall under tokens.\n",
    "\n",
    "Lower case conversion:\n",
    "We want our model to not get confused by seeing the same word with different cases like one starting with capital and one without and interpret both differently. So we convert all words into the lower case to avoid redundancy in the token list. Stop Words removal:\n",
    "When we use the features from a text to model, we will encounter a lot of noise. These are the stop words like the, he, her, etc… which don’t help us and, just be removed before processing for cleaner processing inside the model. With NLTK we can see all the stop words available in the English language.\n",
    "\n",
    "from nltk.corpus import stopwords\"\"\"\n",
    "summary = summarize_text(text)\n",
    "print(\"Final Summary:\", post_process_summary(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf13e69d-dbb8-475c-990e-f8dfd73a6d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd220988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfc6d767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\ASHISHI\n",
      "[nltk_data]     RANJAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "Could not create share link. Missing file: c:\\Users\\ASHISHI RANJAN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\frpc_windows_amd64_v0.3. \n",
      "\n",
      "Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: \n",
      "\n",
      "1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_windows_amd64.exe\n",
      "2. Rename the downloaded file to: frpc_windows_amd64_v0.3\n",
      "3. Move the file to this location: c:\\Users\\ASHISHI RANJAN\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def summarize_text(text, min_words, max_words):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = LsaSummarizer()\n",
    "\n",
    "    # Generate an initial sentence-based summary\n",
    "    summary_sentences = summarizer(parser.document, 10)  # Start with a reasonable number\n",
    "    \n",
    "    # Convert summary to a list of words\n",
    "    summary_text = \" \".join(str(sentence) for sentence in summary_sentences)\n",
    "    words = summary_text.split()\n",
    "\n",
    "    # Adjust summary length based on min & max words\n",
    "    num_words = (min_words + max_words) // 2  # Use average for balanced output\n",
    "    num_words = min(len(words), max(1, num_words))  # Ensure within range\n",
    "\n",
    "    final_summary = \" \".join(words[:num_words])  # Trim to word limit\n",
    "    return final_summary\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=summarize_text, \n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Enter Text to Summarize\"),\n",
    "        gr.Slider(50, 20000, step=50, label=\"Min Summary Length (words)\"),\n",
    "        gr.Slider(50, 20000, step=50, label=\"Max Summary Length (words)\")\n",
    "    ], \n",
    "    outputs=\"text\",\n",
    "    title=\"Text Summarizer (LSA) with Word Control\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
